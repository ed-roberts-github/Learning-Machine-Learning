{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypGed08ulqL6"
   },
   "source": [
    "Note the dataset is photos so is to big to run on google colab so needs to be run in jupiter notebook :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "i8n-M-VUmOYA"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "O7ftmnIFmv9b",
    "outputId": "ba07197d-fcc6-49cb-f6f2-335af0169544"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "egIMAgY5m1sJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#only applying transformations to training set to avoid overfitting.\n",
    "#(image augmentation) This code can be seen on keras API\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True) #1./255 is essentially future scaling\n",
    "\n",
    "#note training data is called training_set in dataset directory\n",
    "#using (64,64) as its the final size of images that will be fed into network (bigger means slower)\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    'dataset/training_set',\n",
    "     target_size = (64, 64),\n",
    "     batch_size = 32,\n",
    "     class_mode = 'binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RnlsBNFfo95g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#need to apply the same future scaling but don't want to transform the images as they are new fresh images\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ETSD8vK3ps7G"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Nt7CBQRJqCcc"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu', input_shape = [64,64,3]))\n",
    "#kernal size is the size of the feature detectors\n",
    "#again using rectifier function\n",
    "#64,64,3 the 3 says its coloured as RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tIfVmGs2q2v3"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))\n",
    "#pool size is the size of the frame which is being pooled over\n",
    "#strides is obvs the stride length taken by the pool area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IsmfvUt9rg2O"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hui3N84vrqNz"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QZunIY05r1YW"
   },
   "outputs": [],
   "source": [
    "#same as with ANN now\n",
    "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FQXe9PS5sLfx"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid')) \n",
    "#binary output so only need one final neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "q2OJq0rXsj9X"
   },
   "outputs": [],
   "source": [
    "#same as in last example with binary output\n",
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cws3HKPps5jj",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 69s 273ms/step - loss: 0.6774 - accuracy: 0.5696 - val_loss: 0.6044 - val_accuracy: 0.6700\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 67s 268ms/step - loss: 0.6069 - accuracy: 0.6651 - val_loss: 0.5923 - val_accuracy: 0.6775\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 63s 253ms/step - loss: 0.5745 - accuracy: 0.7026 - val_loss: 0.5232 - val_accuracy: 0.7460\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 67s 266ms/step - loss: 0.5364 - accuracy: 0.7329 - val_loss: 0.5088 - val_accuracy: 0.7505\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 63s 253ms/step - loss: 0.5132 - accuracy: 0.7530 - val_loss: 0.5366 - val_accuracy: 0.7360\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 0.4922 - accuracy: 0.7567 - val_loss: 0.4760 - val_accuracy: 0.7805\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 68s 270ms/step - loss: 0.4759 - accuracy: 0.7710 - val_loss: 0.4886 - val_accuracy: 0.7690\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 66s 262ms/step - loss: 0.4660 - accuracy: 0.7788 - val_loss: 0.4677 - val_accuracy: 0.7735\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 68s 270ms/step - loss: 0.4472 - accuracy: 0.7847 - val_loss: 0.5056 - val_accuracy: 0.7535\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 0.4320 - accuracy: 0.7961 - val_loss: 0.4874 - val_accuracy: 0.7750\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 62s 248ms/step - loss: 0.4170 - accuracy: 0.8084 - val_loss: 0.4576 - val_accuracy: 0.7875\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.4069 - accuracy: 0.8130 - val_loss: 0.4620 - val_accuracy: 0.7860\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 62s 249ms/step - loss: 0.3916 - accuracy: 0.8227 - val_loss: 0.4721 - val_accuracy: 0.7815\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 67s 267ms/step - loss: 0.3846 - accuracy: 0.8264 - val_loss: 0.5004 - val_accuracy: 0.7920\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 68s 272ms/step - loss: 0.3796 - accuracy: 0.8271 - val_loss: 0.4493 - val_accuracy: 0.7985\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 62s 249ms/step - loss: 0.3577 - accuracy: 0.8420 - val_loss: 0.4643 - val_accuracy: 0.8020\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 0.3439 - accuracy: 0.8514 - val_loss: 0.4656 - val_accuracy: 0.7965\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 63s 253ms/step - loss: 0.3320 - accuracy: 0.8501 - val_loss: 0.4864 - val_accuracy: 0.7970\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 17201s 69s/step - loss: 0.3160 - accuracy: 0.8641 - val_loss: 0.4926 - val_accuracy: 0.7980\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 52s 209ms/step - loss: 0.2900 - accuracy: 0.8741 - val_loss: 0.4796 - val_accuracy: 0.7945\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 0.2880 - accuracy: 0.8756 - val_loss: 0.5237 - val_accuracy: 0.7895\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.2750 - accuracy: 0.8880 - val_loss: 0.5199 - val_accuracy: 0.8025\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.2763 - accuracy: 0.8799 - val_loss: 0.5097 - val_accuracy: 0.7970\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 52s 209ms/step - loss: 0.2603 - accuracy: 0.8944 - val_loss: 0.5187 - val_accuracy: 0.8090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9ed9d8cb20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)\n",
    "#again found epochs by trial and error (note with 25 still takes a while to run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CZfKIo4ittPe"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64,64))\n",
    "\n",
    "#predict method expects a 2d array so need to convert this image into a 2d array\n",
    "test_image = image.img_to_array(test_image)\n",
    "\n",
    "#cnn was trained on batches of images so now we still have to input images as a batch\n",
    "#So adding dimension so it fits what model expects (still only 1 element in bacth)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "\n",
    "#now figuring out what is 1/0 dog/cat\n",
    "training_set.class_indices\n",
    "\n",
    "if (result[0][0] == 1):\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wsg0tPXVv02K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay!! Correct prediction, I also tested on a few other images that all came back as correct,the 80% accuracy on the test set is preety good but not overfitted. If the images augmentation wasn't done then the accuracy would be really high on the training set but the model wouldn't work well on other data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "convolutional_neural_network(CNN).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
