{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of data_preprocessing_tools.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ed-roberts-github/Previous-work/blob/main/data_preprocessing_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKi1njbLHO_o"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9kwFW6HIDwU"
      },
      "source": [
        "dataset = pd.read_csv('Data.csv')\n",
        "x = dataset.iloc[:, :-1].values #this selects all the rows and collums except the last one\n",
        "y = dataset.iloc[:, -1].values #selects just last coll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa_ZlTrhKaZc",
        "outputId": "dda5ce20-d807-482a-9181-822ab8d55879"
      },
      "source": [
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_qiQJPLKdbe",
        "outputId": "2a4d0d23-b544-4994-c272-9b6a467e7c1f"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBa9mxjTIH6u"
      },
      "source": [
        "from sklearn.impute import SimpleImputer #importing a class from impute module\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean') #declaring an onbject\n",
        "imputer.fit(x[:,1:3]) #fit method looking for all missing values in col 1 and 2\n",
        "x[:,1:3] = imputer.transform(x[:,1:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_uLL21iNdTx",
        "outputId": "0d28c726-8817-43d3-dc3b-7f142810f33c"
      },
      "source": [
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nPH7pka7Ux3"
      },
      "source": [
        "#have to change the strings in country into another format, don't just use 0,1,2 as this can be\n",
        "#interpreted wrong by the alogrithm later so instead we make the strins into vectors (1,0,0),(0,1,0),(0,0,1)\n",
        "#this is called OneHot encoding\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#creating object\n",
        "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(), [0] )], remainder='passthrough')\n",
        "#remainder = 'passthrough' ensure we keep the other collumns in x, otherwise we'd only end up with the country col\n",
        "\n",
        "x = np.array(ct.fit_transform(x)) #this transforms the country collumn to the OneHot new one\n",
        "#need to have x as a numpy array, do this by calling np.array()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipLvVQl37081",
        "outputId": "567be9a7-1260-4b5d-9cc3-a42e17e12cd8"
      },
      "source": [
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN_UapMm7Xl2"
      },
      "source": [
        "#don't need onehot encoding here as only 2 options\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder() #nothing in () as only 1 single vector so obvs what needed to be encoded\n",
        "y = le.fit_transform(y) #no need for numpy array as its depedant variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3PKiHWp73L9",
        "outputId": "0b0ac86f-bee1-4a8d-9b34-71797140e8c0"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-4SdyDn8w-U"
      },
      "source": [
        "We split the data set up before doing feature scaling because the test set is meant to be a completely new set of data. We arent supposed to work with the test set while training! We would end up with 'data leakage' on the test set.\n",
        "\n",
        "Feature scaling is done to scale your features so they all have values within the same scale so one doesn't dominated the training algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4WB_TpM9hrd"
      },
      "source": [
        " from sklearn.model_selection import train_test_split\n",
        " x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1) \n",
        " #3rd argument in () is the split size usual done 80% training set\n",
        " #4th argument is just done in this case to select which random split I'll get so it matches\n",
        " #the course split data (so this isn't usual needed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9Ntm0eH_hSq",
        "outputId": "4e6281f9-0cdb-44f1-f99f-9d773108a874"
      },
      "source": [
        "print(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgXJP0d0_hGI",
        "outputId": "e4020dcc-ffbc-4b24-96de-e6364b06fb06"
      },
      "source": [
        "print(x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCDjP_NO_g21",
        "outputId": "aa2bd1d6-77e6-496d-8df6-7b2d3c6bdde2"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCVdGQ-m_gl1",
        "outputId": "3c3cd703-4d09-4288-d647-8b062f90ab8a"
      },
      "source": [
        "print(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulAxaGx2AO8k"
      },
      "source": [
        "Puts all features on the same scale as some ML models can be dominated by some values (this isn't needed for all ML models). \n",
        "There are two main methods of future scaling, Standardisation (values around -3 to +3) or Normalisation (between 0 to 1).\n",
        "\n",
        "Normalisation is recommened when you have a normal distibrution. Standardisation works pertty much all the time, so its recommened to use standardisation. This is the method done below. x_stand = (x-mean(x))/standard deviation(x)\n",
        "\n",
        "You don't have to apply standardisation to dummy variable (the ones which were strings that we turned into vectors) because firstly these values are already between the -3 to 3 range and secondly IF WE DID APPLY FUTURE SCALING WE COULDN'T TELL WHICH DUMMY VARIBALE CORRESPONDS WITH WHICH ORIGINAL VALUE (ie country in this case)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h2pcRqLANdp"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "x_train[:,3:] = sc.fit_transform(x_train[:,3:])\n",
        " #'3:' as we know onehot creates 3 colls at start and so we want to have all collumes after the first.\n",
        "#Fit computes the standard dev and mean of all the values, transfrom applies the formula and gets x_stand\n",
        "#so here we use fit_transfrom\n",
        "\n",
        "x_test[:,3:] = sc.transform(x_test[:,3:])\n",
        "#only applying transform method because features needs same scalar applied as x_train. ie we need to\n",
        "#get the same transformation as that applied to the training set (so we just use transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYJN3xqbE-dD",
        "outputId": "f4456f81-8342-4af0-dba8-ffddd6b8f00f"
      },
      "source": [
        "print(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
            " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
            " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
            " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
            " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
            " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
            " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
            " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pH_hDesFApJ",
        "outputId": "6b2c95cb-7154-475c-b266-48f655f939c7"
      },
      "source": [
        "print(x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
          ]
        }
      ]
    }
  ]
}